---
title: "OpenAlex Data Snapshot Usage"
format:
  pdf:
    toc: false
vignette: >
  %\VignetteIndexEntry{Vignette's Title}
  %\VignetteEngine{quarto::pdf}
  %\VignetteEncoding{UTF-8}
execute: 
  cache: false
---

# Introduction

In this document I will illustrate the use of the OpenAlex data snapshot.

There are two underlying assumptions:

1. It is complex to implement full text search locally (this includes search in title, abstract, and fulltext), and relatively fast / easy to obtain all the `ids` of the results (no further data is needed)
2. filtering of the data, e.g. year ranges, topics, etc, can be done easily locally.

The workflow looks essentially as follows:

1. a fulltext search is done on the OpenAlex API and the `ids`  are retriweved
2. based on these `ids`, the bibliometric data is retrieved from a local snapshot and saved in a local corpus in parquet format.
3. Based on this resulting corpus, further filtering and analysis can be done
4. when an additional fulltext search is needed, it is done again via the OpenAlex API and the `ids` are retrieved and the intersection with the previous corpus determined.

This should lead to a reltively efficient workflow which makes it on the one hand possible to do

- queries which are at the moment not possible due to their length
- use a consistent database of OpenAlex data from a specific data snapshot even when the queries are repeated after a longer time
- make usage of the locak snapshot easier as no implementation of the fulltext search is needed

## Examples

### Getting OpenAlex Snapshot
First step is to get the local snapshot and convert it into parquet format. 

Be aware that you need about 500 GB for the snapshot, and about 600 GB for the parquet database.

The easiest is to use the `Makefile` which is part of this package and can be copied to the current directory by running

```{r}
file.copy(from = system.file("Makefile", package = "openalexPro"), to = ".")
```

The `Makefile` looks as follows:

```{r}
cat(paste0(readLines(system.file("Makefile", package = "openalexPro")), collapse = "\n"))
```

To download the snapshot into the directory `openalex-snapshot` you can run

```{sh}
#| eval: false

make snapshot
```

To convert the snapshot into a parquet database in the directory `arrow` you can run

```{sh}
#| eval: false

make arrow
```

TODO: Deescribe arrow dir

### Getting the `ids` resulting from a, OpenAlex fulltext search

There are multiple ways of achieving this. For a small number the easiest is to use the package `openalexR` and the function `oa_fetch()` which uses the OpenAlex API to get the `ids` of the results.

This can be done as follows:

```{r}

library(openalexR)
ids <- oa_fetch(
    entity = "works",
    output = "tibble",
    title_and_abstract.search = "biodiversity AND conservation AND IPBES",
    options = list(select = "id")
) 

ids |>
    dplyr::arrange(id) |>
    knitr::kable()
```

### Filtering the works out of the data snapshot

We now assume, that the arrow snapshot can be found at

```{r}
works_dir <- file.path("/", "Volumes", "openalex", "arrow", "works")
```


Now we pull these works out of the local data snapshot.  A few remarks at this point:

- It is possible (even likely) that not all `ids` are in the local snapshot, as OpenAlex continuously updates it's database. This is actually an advantage, as the data source, which is the data snapshot, stays constant over the whole project and the resulting numbers do not change over time. 
- In some cases still duplicate `ids` in the dataswet used by OpenAlex and therefore also in the data snapshot. The difference between the duplicates (sometimes even more) are only in the metadata and the underlying document is the same. This can in most cases be neglected, and it is therefore a good idea to remove them. Nevertheless, one can also take a closer look at these. This should eb rectified by OpenAlex in the near future.
- the de-duplication can, at the moemnt, only be done by loading the whole corpus into memory, wherefore it is unsuitable at the moment for large corpora. The implementation via duckdb is planned for the future.

TO CHECK!!!!! 

```{r}


```