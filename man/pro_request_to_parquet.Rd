% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pro_request_to_parquet.R
\name{pro_request_to_parquet}
\alias{pro_request_to_parquet}
\title{Convert JSON files to Apache Parquet files}
\usage{
pro_request_to_parquet(
  json_dir = NULL,
  output_dir = tempfile(fileext = "_parquet"),
  overwrite = FALSE,
  verbose = TRUE,
  delete_input = FALSE,
  jq_path = "jq",
  ROW_GROUP_SIZE = 10000,
  ROW_GROUPS_PER_FILE = 1,
  output_nn = tempfile(fileext = "_parquet_raw")
)
}
\arguments{
\item{json_dir}{The directory of JSON files returned from \code{pro_request(..., json_dir = "FOLDER")}.}

\item{output_dir}{output  directory for the parquet dataset; default: temporary directory.}

\item{overwrite}{Logical indicating whether to overwrite \code{output_dir} and \code{output_nn}.}

\item{verbose}{Logical indicating whether to show a verbose information. Defaults to \code{TRUE}}

\item{delete_input}{Determines if the \code{json_dir} should be deleted afterwards. Defaults to \code{FALSE}.}

\item{jq_path}{Path to the jq executable (default: "jq")}

\item{ROW_GROUP_SIZE}{Only used when \code{normalize_schemata = TRUE}. Maximum number of rows per row group. Smaller sizes reduce memory
usage, larger sizes improve compression. Defaults to \code{10000}.
See: \url{https://duckdb.org/docs/sql/statements/copy#row_group_size}}

\item{ROW_GROUPS_PER_FILE}{Only used when \code{normalize_schemata = TRUE}. Number of row groups to include in each output Parquet file.
Controls file size and write frequency. Defaults to \code{1}
See: \url{https://duckdb.org/docs/sql/statements/copy#row_groups_per_file}}

\item{output_nn}{output  directory for the non-normalized parquet dataset; default: temporary directory.
Mainly for debugging purposes needed.}
}
\value{
The function does returns the output_directory invisibly.
}
\description{
The function takes a directory of JSON files as written from a call to \code{pro_request(..., json_dir = "FOLDER")}
and converts it to a Apache Parquet dataset partitiond by the page.
}
\details{
The function uses DuckDB to read the JSON files and to create the
Apache Parquet files. The function creates a DuckDB connection in memory and
readsds the JSON files into DuckDB when needed. Then it creates a SQL query to convert the
JSON files to Apache Parquet files and to copy the result to the specified
directory.
}
\examples{
\dontrun{
source_to_parquet(json_dir = "json", source_type = "snapshot", output_dir = "arrow")
}
}
