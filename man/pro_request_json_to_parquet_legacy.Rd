% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/legacy.R
\name{pro_request_json_to_parquet_legacy}
\alias{pro_request_json_to_parquet_legacy}
\title{Convert JSON files to Apache Parquet files}
\usage{
pro_request_json_to_parquet_legacy(
  json_dir = NULL,
  corpus = tempfile(fileext = "_corpus"),
  overwrite = FALSE,
  verbose = TRUE,
  delete_input = FALSE
)
}
\arguments{
\item{json_dir}{The directory of JSON files returned from \code{pro_request(..., json_dir = "FOLDER")}.}

\item{corpus}{parquet dataset; default: temporary directory.}

\item{verbose}{Logical indicating whether to show a verbose information. Defaults to \code{TRUE}}

\item{delete_input}{Determines if the \code{json_dir} should be deleted afterwards. Defaults to \code{FALSE}.}

\item{normalize_schemata}{Determines if the schemata should be normalized, i.e. If not,
certain fields might not work, but for some app;licatons this is faster. Defasults to \code{FALSE}}

\item{ROW_GROUP_SIZE}{Only used when \code{normalize_schemata = TRUE}. Maximum number of rows per row group. Smaller sizes reduce memory
usage, larger sizes improve compression. Defaults to \code{10000}.
See: \url{https://duckdb.org/docs/sql/statements/copy#row_group_size}}

\item{ROW_GROUPS_PER_FILE}{Only used when \code{normalize_schemata = TRUE}. Number of row groups to include in each output Parquet file.
Controls file size and write frequency. Defaults to \code{1}
See: \url{https://duckdb.org/docs/sql/statements/copy#row_groups_per_file}}

\item{enrich_corpus}{Determines if the function \code{enrich_parquets()} should be run. Defatults to \code{FALSE}}
}
\value{
The function does not returns the directory with the corpus.
}
\description{
The function takes a directory of JSON files as written from a call to \code{pro_request(..., json_dir = "FOLDER")}
and converts it to a Apache Parquet dataset partitiond by the page.
}
\details{
The function uses DuckDB to read the JSON files and to create the
Apache Parquet files. The function creates a DuckDB connection in memory and
readsds the JSON files into DuckDB when needed. Then it creates a SQL query to convert the
JSON files to Apache Parquet files and to copy the result to the specified
directory.
}
\examples{
\dontrun{
source_to_parquet(json_dir = "json", source_type = "snapshot", corpus = "arrow")
}
}
